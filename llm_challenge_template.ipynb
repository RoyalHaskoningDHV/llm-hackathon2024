{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sbuergers/llm-hackathon/blob/add_gpt4_api/llm_challenge_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vl4Qy5eOhH4_"
   },
   "source": [
    "# Welcome to the Large Language Models Challenge!\n",
    "\n",
    "This is a template notebook. __Copy it__ and start hacking away if you like. We suggest removing the `_template` suffix and replacing it with your team name. If you do not have a team name yet, have a look below. There are already a few useful snippets of code that might help you in your quest, including a random team name generator!\n",
    "\n",
    "But first some pre-requisites. Download the [cpv-master.zip](https://stllmchallenge2024.blob.core.windows.net/data/cpv-master.zip?sp=r&st=2024-09-09T06:03:17Z&se=2024-09-11T14:03:17Z&spr=https&sv=2022-11-02&sr=b&sig=uI7Y00VjgiqcfX96imRFootgC5J2SYDKoDkd%2BVLAGJs%3D) file and install it with the following cell. You will also need to download these three files and put them in the `./data/enriched` folder in your google colab files tab (on the left): [file 1](https://stllmchallenge2024.blob.core.windows.net/data/data_tenderned.csv?sp=r&st=2024-09-09T06:57:54Z&se=2024-09-11T14:57:54Z&spr=https&sv=2022-11-02&sr=b&sig=YMNvh9YrqFctLj7TYQNu%2FTutJ%2FxzzkFI%2FAVFgivlsRg%3D), [file 2](https://stllmchallenge2024.blob.core.windows.net/data/test_data_tenderned_clean.csv?sp=r&st=2024-09-09T06:58:54Z&se=2024-09-11T14:58:54Z&spr=https&sv=2022-11-02&sr=b&sig=W0yBbrllJHGa5etPsiRCFV%2Fz16Khq4pwmRANCZLltrs%3D), [file 3](https://stllmchallenge2024.blob.core.windows.net/data/train_data_tenderned_and_ted_clean.csv?sp=r&st=2024-09-09T06:59:30Z&se=2024-09-12T14:59:30Z&spr=https&sv=2022-11-02&sr=b&sig=cE6WBsHxwjij9%2FM%2FTOYfZAAoFncECUkXJr70qka9wvo%3D). Finally, create an empty `./output/models` folder and the run the following two cells.\n",
    "\n",
    "In google colab, on the left, there is a files tab, for the code here to work you need to add the two above files there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cpv-master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dill as pickle\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "def assess_model_performance(y_hat, y_test):\n",
    "    acc = metrics.accuracy_score(y_test, y_hat)\n",
    "    prec = metrics.precision_score(y_test, y_hat, average=\"weighted\")\n",
    "    rec = metrics.recall_score(y_test, y_hat, average=\"weighted\")\n",
    "    f1 = metrics.f1_score(y_test, y_hat, average=\"weighted\")\n",
    "    print(\"accuracy: {0:.2g}\".format(acc))\n",
    "    print(\"precision: {0:.2g}\".format(prec))\n",
    "    print(\"recall: {0:.2g}\".format(rec))\n",
    "    print(\"f1-score: {0:.2g}\".format(f1))\n",
    "\n",
    "\n",
    "class LemmaTokenizerNoStopWords:\n",
    "    \"\"\"Performas lemmatization and excludes stopwords using spacy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"nl_core_news_sm\")\n",
    "        self.stopwords = self.nlp.Defaults.stop_words\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        tokens = [token.lemma_.lower().strip() for token in self.nlp(doc)]\n",
    "        return [t for t in tokens if t not in self.stopwords]\n",
    "\n",
    "\n",
    "def remove_categories_not_in_tenderned(\n",
    "    data: pd.DataFrame,\n",
    "    data_path: str,\n",
    "    y_label: str = 'afdeling',\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Using the Tenderned dataset as ground truth, remove all categories\n",
    "    at the y_label level that are not in it (e.g. when using TED data as well).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Input data (e.g. from Tenderned + TED)\n",
    "    data_path : str\n",
    "    y_label : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_pruned : pd.DataFrame\n",
    "        Output data containing only labels from tenderned in y_label column\n",
    "    \"\"\"\n",
    "    data_tenderned = pd.read_csv(data_path)\n",
    "    tenderned_labels = data_tenderned.loc[:, y_label].unique()\n",
    "    data = data.loc[data.loc[:, y_label].isin(tenderned_labels), :]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_classes_with_fewer_than_n_observations(\n",
    "    df: pd.DataFrame, y_label: str, n: int = 49,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"If we perform classificaiton at the groep, class, category or\n",
    "    description level, we often do not have sufficient observations to\n",
    "    properly train or even split into train and test set. Remove those entries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input data\n",
    "    n : int (default=49)\n",
    "        Minimum number of class observations to keep class in dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Data without observations from classes with less than n entries\n",
    "    \"\"\"\n",
    "    nrows = df.shape[0]\n",
    "    cpv_freq_table = df[y_label].value_counts()\n",
    "    cpv_codes_to_drop = (\n",
    "        pd.DataFrame(cpv_freq_table).index[cpv_freq_table < n].values\n",
    "    )\n",
    "    df = df.loc[~df[y_label].isin(cpv_codes_to_drop), :]\n",
    "    df = df.dropna(subset=y_label, axis=0)\n",
    "\n",
    "    logging.info(\n",
    "        f\"Removed {nrows-df.shape[0]} entries, because they\"\n",
    "        f'have less than {n} observations in target column \"{y_label}\"'\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_classes_with_nan(df: pd.DataFrame, y_label: str) -> pd.DataFrame:\n",
    "    \"\"\"Remove all entries with nan in target variable\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input data\n",
    "    y_label : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Data without nans in target variable\n",
    "    \"\"\"\n",
    "    return df.dropna(axis=0, subset=y_label)\n",
    "\n",
    "\n",
    "def fit_and_cache_simple_pipeline(\n",
    "    filename: str, y_label: str, n_min_observations: int=49, use_zwolle_codes: bool=False\n",
    "):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        max_df=0.5,\n",
    "        min_df=5,\n",
    "        tokenizer=LemmaTokenizerNoStopWords(),\n",
    "        token_pattern=None,\n",
    "        max_features=None,\n",
    "    )\n",
    "    estimator = RidgeClassifier(alpha=0.75, solver=\"auto\")\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"vectorizer\", vectorizer),\n",
    "            (\"estimator\", estimator),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model_path = Path(\"./output/models\")\n",
    "    model_name = f\"small_model_{filename}_lvl_{y_label}_minobs_{n_min_observations}.pkl\"\n",
    "    \n",
    "    # Fit model on train set\n",
    "    train_data_clean = pd.read_csv(f\"./data/enriched/train_{filename}_clean.csv\")\n",
    "    X_train = remove_categories_not_in_tenderned(\n",
    "        train_data_clean.copy(),\n",
    "        data_path=\"./data/enriched/data_tenderned.csv\",\n",
    "        y_label=y_label,\n",
    "    )\n",
    "    X_train = remove_classes_with_fewer_than_n_observations(\n",
    "        X_train,\n",
    "        y_label,\n",
    "        round(n_min_observations * 0.6),\n",
    "    )\n",
    "    X_train = remove_classes_with_nan(X_train, y_label)\n",
    "    y_train = X_train.loc[:, y_label].copy()\n",
    "    \n",
    "    pipeline.fit(X_train.loc[:, 'Korte beschrijving aanbesteding'].values, y_train.values)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_data_clean = pd.read_csv(f\"./data/enriched/test_data_tenderned_clean.csv\")\n",
    "\n",
    "    X_test = test_data_clean.copy().loc[:, 'Korte beschrijving aanbesteding'].tolist()\n",
    "    y_test = test_data_clean.loc[:, y_label].copy()\n",
    "    \n",
    "    y_hat = pipeline.predict(X_test)\n",
    "    \n",
    "    try:\n",
    "        assess_model_performance(y_hat, y_test)\n",
    "    except:\n",
    "        print('Cannot perform model assessment')\n",
    "\n",
    "    # Cache model\n",
    "    with open(model_path / model_name, \"wb\") as file:\n",
    "        pickle.dump(pipeline, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the classical model training\n",
    "\n",
    "Now we are ready to run the training of the classical ML models at different cpv-code levels. This code takes a long time to run, so consider starting only with the `Omschrijving` level. I am currently fitting all models on my laptop, once done I will step by step upload the pickle files to the blob store and provide you with the download links. Then you can add them to the `./output/models` folder directly from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data_tenderned_and_ted'\n",
    "for y_label in ['afdeling', 'groep', 'klasse', 'categorie', 'Omschrijving']:\n",
    "    print(f'Fit model at the {y_label} level.')\n",
    "    fit_and_cache_simple_pipeline(filename, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEfibUJykXI8",
    "outputId": "5ddd8f41-aa98-46e8-c00c-fcdcc8ddb82c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated team name:  noble_monkeys\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Function to generate a random team name\n",
    "def generate_team_name():\n",
    "    adjectives = [\"Agile\", \"Brave\", \"Clever\", \"Daring\", \"Energetic\", \"Fearless\", \"Gallant\", \"Heroic\", \"Innovative\", \"Jovial\", \"Keen\", \"Loyal\", \"Mighty\", \"Noble\", \"Optimistic\", \"Persistent\", \"Quick\", \"Resilient\", \"Strong\", \"Tenacious\", \"Unyielding\", \"Valiant\", \"Wise\", \"Xenial\", \"Youthful\", \"Zealous\"]\n",
    "    nouns = [\"Antelopes\", \"Bears\", \"Cheetahs\", \"Dolphins\", \"Elephants\", \"Foxes\", \"Giraffes\", \"Hawks\", \"Iguanas\", \"Jaguars\", \"Kangaroos\", \"Lions\", \"Monkeys\", \"Nightingales\", \"Owls\", \"Penguins\", \"Quails\", \"Rabbits\", \"Snakes\", \"Tigers\", \"Unicorns\", \"Vultures\", \"Wolves\", \"Xiphosuran\", \"Yaks\", \"Zebras\"]\n",
    "    adjective = random.choice(adjectives)\n",
    "    noun = random.choice(nouns)\n",
    "    return adjective.lower() + \"_\" + noun.lower()\n",
    "\n",
    "# Generate a random team name\n",
    "team_name = generate_team_name()\n",
    "print(\"Generated team name: \", team_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Liknda-Hqffm"
   },
   "source": [
    "## Why not scrape some additional info from the web?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKHwEUtGgtNt"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Function to extract first text paragraph after first h2 tag from website.\n",
    "# (If you use https://cpvcodes.eu/en/{cpv_code}-cpv/, you get a detailed description for that code)\n",
    "def extract_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the first <h2> tag\n",
    "    first_h2 = soup.find('h2')\n",
    "\n",
    "    if first_h2:\n",
    "        # Find the next <p> tag after the first <h2> tag\n",
    "        first_p_after_h2 = first_h2.find_next('p')\n",
    "        if first_p_after_h2:\n",
    "            return first_p_after_h2.text\n",
    "        else:\n",
    "            return \"No <p> tag found after the first <h2> tag.\"\n",
    "    else:\n",
    "        return \"No <h2> tag found in the webpage.\"\n",
    "\n",
    "\n",
    "# Test the function\n",
    "url = \"https://cpvcodes.eu/en/03211000-cpv/\"\n",
    "print(extract_text(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eq7voKtcg_yn",
    "outputId": "027f32b7-6597-4b37-827c-94be5fb41deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cereals category includes various types of grains that are commonly used as food sources. These grains are essential ingredients in many food products and are widely consumed worldwide. The subcategories within this category consist of Wheat, Maize (corn), Rice, Barley, Rye, Oats, Malt, and Grain products. Wheat is a versatile grain used in bread, pasta, and pastries. Maize is primarily used for animal feed and as a raw material for various food products. Rice is a staple food in many cultures and is consumed in various forms. Barley is often used in brewing and as a nutritious grain. Rye and oats are commonly used in bread, cereals, and other baked goods. Malt is a key ingredient in beer production. Grain products encompass a wide range of processed goods derived from grains, such as flour, cereals, and snacks.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25B_EQh_hBZe",
    "outputId": "c897b3f8-303b-4903-95c4-7b719e614766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.37.1-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Downloading openai-1.37.1-py3-none-any.whl (337 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.37.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHCDlIEDs-tC"
   },
   "source": [
    "## We use a GPT-4o mini API from openai in Azure AI services.\n",
    "\n",
    "Be aware that we pay for every 1000 tokens sent (€0.00014)\tand received (€0.0006)!\n",
    "\n",
    "Feel free to adjust the below code for your hackathon project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEXuh203s7Wm",
    "outputId": "fe87f1ac-8772-47a7-f4bc-41e204f3d794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-9qcw4TRasAVkDHQoDL80WmGs9GVwV\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"De Common Procurement Vocabulary (CPV) code voor het bouwen van een brug uit beton valt onder de categorieën voor bouw- en constructiewerkzaamheden. De specifieke CPV-code voor bruggenbouw is:\\n\\n- **45221100-3** - Constructiewerk voor bruggen\\n\\nDeze code is niet gespecificeerd voor het materiaal beton, maar beschrijft in brede termen het constructiewerk voor bruggen. Voor nog meer specificiteit kan het nuttig zijn om aanvullende codes te gebruiken die relevante aspecten van betonconstructies aanduiden:\\n\\n- **45223200-8** - Bouwwerkzaamheden voor constructies van beton\\n\\nHet combineren van deze codes in uw aanbestedingsdocumenten kan helpen om duidelijkheid te verschaffen over de specifieke aard van de werkzaamheden die moeten worden uitgevoerd.\\n\\nHet is altijd goed om de specifieke context van uw project te overwegen en eventueel andere relevante codes toe te passen om volledig te voldoen aan de vereisten van het project.\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"protected_material_code\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"protected_material_text\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1722329752,\n",
      "  \"model\": \"gpt-4o-2024-05-13\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": \"fp_abc28019ad\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 197,\n",
      "    \"prompt_tokens\": 21,\n",
      "    \"total_tokens\": 218\n",
      "  },\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "# may change in the future\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
    "api_version = \"2024-06-01\"\n",
    "\n",
    "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    api_key=userdata.get('AZURE_OPENAI_API_KEY'),\n",
    "    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "    azure_endpoint=userdata.get('OPENAI_ENDPOINT'),\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"hackathonllms2024-gpt4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Wat is de CPV code om een brug te bouwen uit beton?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(completion.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the solution\n",
    "\n",
    "Please, make sure that your solution can support the following. It should be able to take a dataframe with the columns as provided in the example below, and return CPV-codes as predictions - one for each input row. This is necessary to run the scoring algorithm. You can get the scoring data [here](https://stllmchallenge2024.blob.core.windows.net/data/scoring_dataset.csv?sp=r&st=2024-09-08T13:38:32Z&se=2024-09-11T21:38:32Z&spr=https&sv=2022-11-02&sr=b&sig=m19SyuKrSEl1nYqaV1z00Cg9MYmSwlBfX2kSuCFd%2FHY%3D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pd.read_csv(\"scoring_dataset.csv\")\n",
    "X_test = pd.DataFrame(\n",
    "    {\n",
    "        \"description\": [\"tender description 1\", \"tender description 2\", \"tender description 3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "def predict(X: pd.DataFrame) -> pd.DataFrame:\n",
    "  # Take a DataFrame with \"description\" column as input, use it to predict CPV codes\n",
    "  # and put them in a \"prediction\" column of an output dataframe!\n",
    "  return pd.DataFrame({\"prediction\": [\"12345678-0\", \"12345678-0\", \"12345678-0\"]})\n",
    "\n",
    "predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/RoyalHaskoningDHV/llm-hackathon2024@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_hackathon.scoring import score_solution\n",
    "\n",
    "score_solution(predictions)  # The first value is the F1 score, the second value is accuracy weighted by detail and importance to zwolle"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOtJjixWSnnJaxXpkrRctio",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
